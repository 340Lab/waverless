# Waverless 项目关键设计笔记

## 1. 函数执行上下文设计

### 1.1 基础结构
- `FnExeCtx`: 私有的基础结构体，包含函数执行的基本信息
  ```rust
  struct FnExeCtx {
      pub app: String,
      pub app_type: AppType,
      pub func: String,
      pub func_meta: FnMeta,
      pub req_id: ReqId,
      pub event_ctx: EventCtx,
      pub res: Option<String>,
      pub sub_waiters: Vec<JoinHandle<()>>,
      _dummy_private: (),
  }
  ```

### 1.2 公开特化类型
- `FnExeCtxAsync` 和 `FnExeCtxSync`:
  - 异步执行上下文支持 Jar、Wasm、Native 类型，包含子任务支持和完整的性能监控和日志。
  - 同步执行上下文仅支持 Native 类型，不支持子任务，包含基本的性能监控和日志。

### 1.3 类型安全
- `FnExeCtxAsyncAllowedType` 和 `FnExeCtxSyncAllowedType`:
  - 异步允许的类型 (Jar, Wasm, Native)
  - 同步允许的类型 (仅 Native)
  - 通过 `TryFrom<AppType>` 在编译时强制类型安全

## 2. 实例管理设计

### 2.1 实例类型与管理器
- `Instance` 和 `InstanceManager`:
  - `Instance` 包含 Owned、Shared 和 Native 类型。
  - `InstanceManager` 管理应用实例和运行时函数上下文。
  ```rust
  pub enum Instance {
      Owned(OwnedInstance),
      Shared(SharedInstance),
      Native(NativeAppInstance),
  }
  
  pub struct InstanceManager {
      pub app_instances: SkipMap<String, EachAppCache>,
      pub instance_running_function: DashMap<String, UnsafeFunctionCtx>,
  }
  ```

### 2.2 运行时函数上下文
- `UnsafeFunctionCtx`:
  - 包含 Sync 和 Async 类型，分别对应 `FnExeCtxSync` 和 `FnExeCtxAsync`。

## 3. 关键修改记录

### 3.1 同步/异步执行流程优化与错误处理增强
- 简化 `finish_using`，移除不必要的异步版本，统一使用同步实现。
- 添加同步版本的 `load_instance_sync`，仅支持 Native 类型。
- 优化 `execute_sync` 中的异步调用处理，统一性能监控和日志记录格式。
- 添加 `UnsupportedAppType` 错误类型，完善同步执行时的类型检查。

## 4. 待办事项
- [x] 考虑添加同步版本的 `load_instance`
- [ ] 优化 `execute_sync` 中的异步-同步转换
- [ ] 完善错误处理和日志记录

## 5. 核心设计原则

### 5.1 基础原则与 View 模式设计规则
- 同步/异步分离，类型安全，性能监控，资源管理。
- View 生成：
  - View 结构体和 `LogicalModule` trait 的实现由宏生成。
  - 只需实现 `inner_new` 函数，使用 `logical_module_view_impl!` 生成访问函数。
  - 每个需要访问的模块都需要单独的 impl 宏调用。

### 5.2 去掉 #[derive(LogicalModule)] 的原因和注意事项
- 实现特定功能：根据需求在 `DataGeneralView` 中实现特定功能，检查冲突。
- `inner` 字段的管理：由宏管理，不能直接操作，通过宏生成的接口使用。
- 错误分析：去掉派生后，仔细分析和解决可能出现的错误。

## 6. msg_pack 消息封装

### 6.1 基本原则与实现示例
- 使用 `msg_pack.rs` 中的宏实现 trait，使用 `define_msg_ids!` 管理消息类型。
- 通过 `RPCReq` trait 定义请求-响应关系。
  ```rust
  define_msg_ids!(
      (proto::sche::BatchDataRequest, pack, { true }),
      (proto::sche::BatchDataResponse, _pack, { true })
  );
  
  impl RPCReq for proto::sche::BatchDataRequest {
      type Resp = proto::sche::BatchDataResponse;
  }
  ```

### 6.2 最佳实践
- 新增消息类型时：在 `define_msg_ids!` 中添加定义，实现 `RPCReq` trait。
- 使用消息时：使用 `RPCCaller` 和 `RPCHandler`，遵循统一的错误处理。

## 7. Waverless 代码规范核心规则

### 7.0 最高优先级规则
- 在没有经过明确允许的情况下,不要擅自开始操作
- 必须等待用户明确指示后再进行修改
- 在进行任何修改前,先提出修改方案并等待确认
- 有明确指令的情况下,不要擅自做其他操作
- 删除代码时必须说明:
  - 被删除代码的原有功能和作用
  - 删除的具体原因
  - 删除可能带来的影响
- 修改代码时必须:
  - 先提出完整的修改方案
  - 说明每处修改的原因和影响
  - 等待用户确认后再执行
  - 严格按照确认的方案执行,不额外修改
  - 如需额外修改,必须重新提出方案并确认
- 修改规则文件时必须:
  - 确认文件名必须是 `.cursorrules`
  - 确认文件以 "# Waverless 项目关键设计笔记" 开头
  - 确认包含完整的设计笔记结构
  - 确认包含所有规则章节(1-7)
  - 修改前使用搜索工具确认是正确的规则文件
  - 修改前检查文件的完整内容
  - 修改前确认修改的具体位置
  - 只修改规则相关部分
  - 保持其他内容不变
  - 保持文档结构完整
- 执行命令时必须:
  - 先提出执行计划
  - 说明执行目的和预期结果
  - 等待用户确认后再执行
  - 记录执行结果和遇到的问题
  - 如遇问题,提出解决方案并等待确认
  - 例外情况：
    1. 编译命令（sudo -E $HOME/.cargo/bin/cargo build）可以直接执行，无需等待确认
    2. 编译命令必须将输出重定向到 compilelog 文件
    3. 编译命令执行后必须分析结果并更新 review.md

- 编译验证规则：
  - 当用户要求检查编译状态时：
    1. 必须立即执行实际的编译命令，无需等待确认
    2. 禁止仅查看历史编译日志
    3. 必须使用正确的编译命令：`sudo -E $HOME/.cargo/bin/cargo build 2>&1 | tee compilelog`
    4. 必须等待编译完成并分析结果
    5. 必须将编译结果记录到 review.md 中
  - 编译执行前必须：
    1. 确认已经在 review.md 中记录了执行计划
    2. 确认编译环境已经准备就绪
    3. 确认使用了正确的编译命令和参数
  - 编译执行后必须：
    1. 分析编译输出中的每个错误和警告
    2. 更新 review.md 中的任务状态
    3. 如果发现新的错误，创建相应的任务记录
  - 禁止行为：
    1. 禁止在没有执行编译的情况下判断编译状态
    2. 禁止仅根据历史记录回答编译相关问题
    3. 禁止忽略编译警告
    4. 禁止在编译失败时不更新任务状态

- 编译后问题处理规则：
  1. 每次编译完成后，如果发现新的问题：
     - 必须先暂停当前操作
     - 立即在 review.md 中记录新问题
     - 对新问题进行完整的分析记录
     - 等待用户确认后再继续处理
  2. 禁止在发现新问题后未经记录就直接处理
  3. 禁止在未经用户确认的情况下处理新问题
  4. 每个新问题必须包含：
     - 与父问题的关系分析
     - 问题的具体表现和影响
     - 初步的解决方案建议
     - 预期的处理步骤
  5. 违反以上规则的行为将被拒绝执行

- review.md 使用规则：
  - 在执行任何操作前必须：
    1. 先检查 review.md 文件是否存在
    2. 阅读完整的 review.md 内容
    3. 理解当前任务的上下文和父问题
    4. 在合适的位置添加新的任务记录
  
  - 更新位置确定原则：
    1. 必须仔细分析当前对话正在处理的具体问题
    2. 找到该问题在 review.md 中的对应位置
    3. 将新内容添加到该问题的相关位置
    4. 禁止简单地追加到文件末尾
    5. 如果找不到明确的对应位置，必须先在对应任务描述下标记为 (working) 并询问用户确认
    6. 对于正在计划或执行中的任务，必须标记为 (working)；同一时间系统中只允许存在一个 (working) 状态的任务记录。如果发现多个 (working) 标记，必须暂停后续操作，并等待用户确认后再统一标记
  
  - 任务记录必须遵循以下格式：
    ```markdown
    - 任务：[任务描述]
      - 分析：
        - 父问题相关性：
          1. 父问题：[引用具体的父问题]
          2. 相关性：[说明与父问题的关系]
          3. 必要性：[说明为什么需要解决]
          4. 优先级：[说明优先级和原因]
        
        - 当前问题：
          1. [具体问题点1]
          2. [具体问题点2]
          ...
        
        - 修改计划：
          1. [具体步骤1]
          2. [具体步骤2]
          ...
      
      - 执行记录：
        - 已完成：
          - [已完成的步骤1]
          - [已完成的步骤2]
        
        - 遇到的问题：
          - 问题1：[问题描述]
            - 解决方案：[方案描述]
            - 解决过程：[过程记录]
    ```
  
  - 任务状态管理：
    1. 新任务必须添加在未完成任务的最前面
    2. 已完成任务必须标记为 (done)
    3. 已完成任务必须移到未完成任务后面
    4. 子任务必须保持正确的缩进层级
    5. 任务完成状态必须实时更新
  
  - 强制执行要求：
    1. 禁止在未更新 review.md 的情况下执行任何操作
    2. 禁止在未经确认的情况下修改已有任务记录
    3. 禁止删除任何历史记录
    4. 必须在每次操作前后更新执行记录
    5. 必须在遇到问题时立即记录
    6. 必须在解决问题后更新解决方案
    7. 违反以上规则的操作将被拒绝执行

- 执行计划必须：
  1. 在执行任何操作前,必须先在 review.md 中记录执行计划
  2. 执行计划必须包含：
     - 任务描述和目标
     - 父问题相关性分析
     - 当前问题分析
     - 具体执行步骤
     - 预期结果
     - 可能的风险
     - 验证方法
  3. 执行计划必须遵循 review.md 的格式要求：
     - 新计划添加在未完成任务的最前面
     - 使用正确的缩进和层级
     - 包含完整的分析和计划部分
  4. 执行过程必须：
     - 严格按照计划执行
     - 实时记录执行结果
     - 遇到问题时立即记录
     - 完成后更新任务状态
  5. 禁止在没有执行计划的情况下：
     - 执行任何命令
     - 修改任何文件
     - 进行任何操作
  6. 如需修改计划：
     - 必须先记录原计划的问题
     - 提出新的计划
     - 等待确认后再继续

### 7.1 文档维护与代码组织原则
- 文档压缩原则：保持无损压缩，合并重复内容，简化表述，重构文档结构。
- 文档更新规则：确认信息完整性，保留技术细节，使用清晰结构展示信息。
- 代码组织规则：宏生成的访问函数直接使用，非 pub 函数只在一个地方定义，View 负责核心实现，具体模块负责自己的功能，通过 View 访问其他模块。

### 7.2 代码修改原则

#### 7.2.1 问题解决原则
- 仅解决当前 review 中关注的问题和遇到的子问题
- 解决问题前必须先写出解决方案的规划：
  1. 分析问题的根本原因
  2. 列出可能的解决方案
  3. 评估每个方案的优缺点
  4. 选择最优方案并说明原因
  5. 列出具体的实施步骤
  6. 考虑可能的风险和应对措施


- 不随意删除或修改已有的正确实现
- 不在多处实现同一功能
- 保持代码结构清晰简单
- 修改前先理解设计原则

#### 异步任务处理原则
- 分析生命周期和所有权需求
- 避免盲目克隆，只克隆必要数据
- 考虑类型特征（如 P2PModule 的轻量级 Clone）
- 评估替代方案

```rust
// 反例：过度克隆
let p2p = self.p2p().clone();        // 不必要，P2PModule 本身就是轻量级的
let data_general = self.data_general().clone();  // 不必要，同上

// 正例：按需克隆
let split_info = split.clone();  // 必要，因为来自临时变量的引用
```

分析要点：
- 使用场景：确认异步任务中的实际需求
- 类型特征：检查是否已实现轻量级 Clone
- 生命周期：特别关注临时变量引用
- 替代方案：考虑其他实现方式

### 7.3 错误与正确示例
- 错误示例：手动实现已有的宏生成函数，在两个地方都实现同一个函数，过度修改已有代码结构，有损压缩文档内容。
- 正确示例：使用宏生成的访问函数，在合适的位置添加新功能，遵循已有的代码组织方式，保持文档的完整性和准确性。

### 7.4 异步任务变量处理规范

#### 1. 变量分析原则
- 生命周期分析：确定变量在异步任务中的生存期
- 所有权需求：判断是否需要克隆或移动所有权
- 类型特征：考虑变量的类型特性（如 Clone、Send、'static 等）
- 数据共享：评估是否需要在多个任务间共享数据

#### 2. 克隆策略
必须克隆的情况：
- 临时变量引用：`split_info.clone()`（来自迭代器）
- 多任务共享：`unique_id.clone()`（多个任务需要）
- 部分数据：`data_item.clone_split_range()`（只克隆需要的范围）

不需要克隆的情况：
- 值类型复制：`version`（直接复制即可）
- 已实现 Copy：基本数据类型
- 单一任务使用：不需要在多个任务间共享的数据

#### 3. View 模式使用规范
基本原则：
- View 本身已经是完整引用：不需要额外的 view 字段
- 异步任务中使用：`self.clone()`
- 模块访问：通过 view 直接访问其他模块

示例代码：
```rust
// 正确示例
let view = self.clone();  // View 本身克隆
let resp = view.data_general().rpc_call_write_once_data...

// 错误示例
let view = self.view.clone();  // 错误：不需要额外的 view 字段
let data_general = self.data_general().clone();  // 错误：不需要单独克隆模块
```

#### 4. 异步任务数据处理检查清单
- [ ] 是否只克隆必要的数据？
- [ ] 临时变量是否正确处理？
- [ ] View 的使用是否符合规范？
- [ ] 是否避免了重复克隆？
- [ ] 数据共享策略是否合理？

#### 5. 常见场景示例

1. 批量数据处理：
```rust
// 正确处理临时变量和部分数据
let split_info = split_info.clone();  // 临时变量必须克隆
let data_item = data_item.clone_split_range(range);  // 只克隆需要的部分
let view = self.clone();  // View 克隆用于异步任务
```

2. 并发任务处理：
```rust
// 使用信号量和数据共享
let semaphore = Arc::new(Semaphore::new(MAX_CONCURRENT));
let view = self.clone();  // 一次克隆，多处使用
for node_id in nodes {
    let permit = semaphore.clone();
    let view = view.clone();  // View 在任务间共享
    tokio::spawn(async move { ... });
}
```

### 7.3 变量类型难分辨的情况

#### 7.3.1 Proto生成的Rust类型
1. proto中的普通字段在Rust中的表现:
   - proto中的 `string file_name_opt = 1` 生成的是普通 `String` 类型，而不是 `Option<String>`
   - proto中的 `bool is_dir_opt = 2` 生成的是普通 `bool` 类型，而不是 `Option<bool>`
   - 字段名带 `_opt` 后缀不代表它在Rust中是 `Option` 类型

2. proto中的message嵌套在Rust中的表现:
   - `DataItem` 中的 `oneof data_item_dispatch` 在Rust中是一个字段
   - 访问路径是: `data.data_item_dispatch` 而不是 `data.data.data_item_dispatch`
   - `Option<DataItem>` 需要先 `unwrap()` 才能访问其内部字段

#### 7.3.2 容易混淆的类型转换
1. proto生成的类型和标准库类型的关系:
   - proto生成的 `String` 字段不能直接用 `unwrap_or_default()`
   - proto生成的 `bool` 字段不能直接用 `unwrap_or()`

### 7.5 思维方式原则
- 思维优先于行动：
  - 在开始任何操作前，先理解"为什么"而不是"怎么做"
  - 确保完全理解当前上下文中的所有信息
  - 避免机械性思维和跳过思考的行为模式
  - 对于涉及代码逻辑的命令，必须先阅读和理解相关代码，再执行命令
  - 当需要复用或参考现有代码逻辑时，必须先在项目中查找并理解相关实现
  - 在理解代码时，需要关注：
    - 代码的执行流程和依赖关系
    - 数据结构和状态管理方式
    - 错误处理和异常情况的处理方式

- 代码分析记录原则：
  - 在修改任何代码之前，必须在 review.md 中记录完整的代码分析：
    1. 问题代码：截取导致问题的具体代码片段
    2. 上下文代码：截取理解问题所需的相关代码
    3. 问题成因：详细分析问题的具体原因
    4. 修复方案：说明如何修复以及为什么这样修复
    5. 修改验证：列出验证修改正确性的方法
  - 分析记录必须：
    - 使用代码块格式展示代码
    - 保持代码片段的完整性和可读性
    - 确保分析逻辑清晰
    - 说明修改的影响范围

- 父问题相关性分析：
  - 在开始分析任何问题之前，必须首先进行父问题相关性分析
  - 分析步骤：
    1. 确认当前问题的父问题是什么
    2. 回溯父问题的执行计划和记录
    3. 判断当前问题是否是父问题引起的
    4. 确认解决当前问题是否必要且有助于解决父问题
  - 分析结果必须包含：
    1. 父问题的明确引用
    2. 相关性的具体分析
    3. 解决必要性说明
    4. 优先级判断
  - 如果当前问题与父问题无关：
    1. 记录分析结果
    2. 暂时搁置该问题
    3. 继续专注于父问题的解决

- 内化规则：
  - 把规则视为思维框架而不是外部约束
  - 养成先检查当前上下文的习惯
  - 避免在已有信息的情况下去外部搜索
- 关注本质：
  - 理解问题的根本原因比立即解决问题更重要
  - 分析失误的思维模式而不是简单记住正确操作
  - 把经验转化为思维方式而不是操作步骤

## 8. 代码评审与修改文档规则

### 8.1 修改计划与记录要求
- 每次修改代码前：
  1. 必须查看项目根目录的 `review.md` 文件
  2. 根据现有内容确定修改计划的位置和层级
  3. 在对应位置添加修改计划
  4. 使用 markdown 格式记录，保持层级结构清晰

### 8.2 文档结构规范
- 所有修改记录必须使用以下简化的问题树结构：
  ```markdown
  - 任务/问题：xxxx
    - 分析：xxxx
    - 计划任务1：xxxx
      新问题1：xxxx
      - 分析：xxxx
      - 计划任务3：xxxx
        已完成
        
    - 计划任务2：xxxx 
      已完成
  ```

- 结构规则：
  1. 父节点必须是具体的任务或问题描述
  2. 第一个子节点必须是对问题的分析
  3. 后续子节点是具体的计划任务
  4. 每个计划任务下可以包含新的问题，遵循相同的结构
  5. 已完成的任务标记为"已完成"
  6. 保持缩进层级清晰

- 示例说明：
  ```markdown
  - 任务：修复类型转换错误
    - 分析：当前代码在类型转换时未考虑空值情况
    - 计划任务1：添加空值检查
      新问题：如何处理空值转换失败
      - 分析：需要在转换失败时提供默认值
      - 计划任务：实现 Option 转换
        已完成
        
    - 计划任务2：添加单元测试
      已完成
  ```

### 8.3 记录要求
1. 修改计划必须包含：
   - 修改目的
   - 预期效果
   - 可能的风险
   - 具体步骤

2. 修改过程必须记录：
   - 实际执行的步骤
   - 遇到的每个问题
   - 解决方案和结果

3. 问题记录必须包含：
   - 问题的具体表现
   - 问题的可能原因
   - 尝试的解决方案
   - 最终的解决方案
   - 预防措施（如果适用）

### 8.4 维护原则
- 保持文档的实时更新
- 确保问题树结构清晰
- 定期回顾和整理文档
- 记录经验教训和最佳实践

### 8.5 任务识别规则

#### 8.5.1 任务状态判断
1. 完成状态标记：
   - 已完成任务必须标记为 `（done）`
   - 未标记 `（done）` 的任务视为未完成
   - 不使用其他状态标记

2. 任务顺序规则：
   - 文档开头说明：`（顺序：新的在前面；先解决就的未完成的；完成的有标注；问题可能存在子问题）`
   - 新任务添加到未完成任务的最前面
   - 已完成任务移到未完成任务的后面
   - 子任务跟随父任务，保持缩进层级

3. 最老未完成任务识别：
   - 从上到下扫描所有顶级任务
   - 跳过带有 `（done）` 标记的任务
   - 第一个不带 `（done）` 标记的任务即为最老未完成任务
   - 子任务不影响父任务的完成状态判断

4. 任务优先级：
   - 未完成任务按出现顺序表示优先级（越靠后优先级越高）
   - 子任务优先级高于同级后续任务
   - 阻塞性问题优先级最高

#### 8.5.2 任务解析检查清单
在识别和处理任务时，必须检查：
- [ ] 任务是否有 `（done）` 标记
- [ ] 任务是否为顶级任务
- [ ] 是否有未完成的子任务
- [ ] 任务的位置是否符合顺序规则
- [ ] 是否存在阻塞性问题

## 9. 批量数据接口设计

### 9.1 BatchTransfer 设计规范

#### 9.1.1 组件职责定义

1. **数据结构职责划分**
   - BatchTransfer（单个传输任务管理器）必须：
     - 维护单个传输任务的完整状态（unique_id, version, block_type, total_blocks）
     - 使用 DashMap 存储接收到的数据块，确保并发安全
     - 通过 Option<Sender> 管理完成状态通知
     - 负责数据块的接收、验证和重组
   
   - BatchManager（全局传输任务管理器）必须：
     - 使用 DashMap 维护所有进行中的传输任务
     - 使用原子计数器生成唯一的请求序列号
     - 负责传输任务的创建、数据块处理和生命周期管理

2. **函数职责要求**
   - call_batch_data（发送端）必须：
     - 使用固定大小（1MB）进行数据分块
     - 通过 BatchManager 创建传输任务
     - 负责数据块的发送
     - 等待传输完成通知

   - handle_block（接收端）必须：
     - 接收并验证单个数据块
     - 更新传输状态
     - 在接收完所有块时触发完成处理

   - complete（完成处理）必须：
     - 校验所有数据块的完整性
     - 根据类型（内存/文件）重组数据
     - 发送完成通知

#### 9.1.2 数据流转规范

1. **发送流程要求**
   - 必须按照以下顺序执行：
     1. 接收原始数据并验证
     2. 计算分块策略
     3. 创建传输任务
     4. 按序发送数据块

2. **接收流程要求**
   - 必须按照以下顺序处理：
     1. 接收数据块并验证
     2. 存储到对应的 BatchTransfer
     3. 检查完整性
     4. 触发完成处理
     5. 通知发送端

#### 9.1.3 错误处理规范

1. **组件错误处理职责**
   - BatchTransfer 必须处理：
     - 数据块完整性验证错误
     - 数据重组过程错误
   
   - BatchManager 必须处理：
     - 传输任务存在性检查错误
     - 并发访问保护错误
   
   - 调用方必须处理：
     - 网络传输错误
     - 超时错误

2. **错误恢复策略**
   - 必须支持以下错误恢复机制：
     - 单个数据块的重试
     - 传输任务的取消
     - 资源的正确释放

#### 9.1.4 资源管理规范

1. **内存管理**
   - 必须预分配适当的缓冲区大小
   - 必须及时释放不再需要的内存
   - 必须控制并发数据块的最大数量

2. **文件管理**
   - 必须使用唯一的临时文件名
   - 必须在完成后清理临时文件
   - 必须正确处理文件权限

3. **并发控制**
   - 必须使用 DashMap 确保并发安全
   - 必须使用原子操作处理计数器
   - 必须正确管理 channel 资源

### 9.2 批量写入实现

#### 9.2.1 总体流程

1. **数据切分**
   - 内存数据按 1MB 切块
   - 文件数据按 4MB 切块
   - 计算总块数和最后一块大小

2. **任务池初始化**
   - 创建 4 个传输任务槽位
   - 每个任务负责一个数据块的传输
   - 任务完成后自动释放槽位

3. **数据块获取**
   - 空闲任务会请求新的数据块
   - 最多预取 8 个块
   - 超过限制则等待其他块处理完成

4. **传输过程**
   - 任务获取到数据块后开始传输
   - 每个请求包含块索引和数据类型
   - 单个请求超时时间为 30 秒

5. **完成处理**
   - 所有块传输完成后结束
   - 失败的块会重试最多 3 次
   - 重试间隔为 1 秒

#### 9.2.2 接收方处理

1. **数据管理**
   - 复用 get_data 的文件和内存管理逻辑
   - 文件使用 FileManager 管理可变文件
   - 内存使用 MemoryManager 管理内存块

2. **并行写入**
   - 每个数据块作为独立的写入任务
   - 文件写入使用 seek + write 定位写入
   - 内存写入使用偏移量计算地址

3. **并发控制**
   - 使用 RwLock 保护共享资源
   - 文件操作使用 async 文件 I/O
   - 内存操作使用原子操作保证并发安全

4. **状态管理**
   - 记录每个块的写入状态
   - 支持断点续传和重试
   - 完成后更新元数据
   ```

3. **接收方处理**
   ```rust
   struct BatchDataWriter {
       // 文件缓存，使用 unique_id 作为 key
       file_cache: HashMap<Vec<u8>, BatchFileCache>,
       // 内存缓存，使用 unique_id 作为 key
       memory_cache: HashMap<Vec<u8>, BatchMemoryCache>,
   }

   impl BatchDataWriter {
       async fn handle_request(&mut self, req: BatchDataRequest) -> BatchDataResponse {
           let cache = match req.block_type {
               DataBlockType::Memory => &mut self.memory_cache,
               DataBlockType::File => &mut self.file_cache,
           };

           // 获取或创建缓存
           let block_cache = cache.entry(req.unique_id.clone())
               .or_insert_with(|| self.create_cache(req.block_type));

           // 写入数据块
           match block_cache.write_block(req.block_index, req.data).await {
               Ok(()) => BatchDataResponse {
                   request_id: req.request_id,
                   success: true,
                   error_message: String::new(),
                   version: req.version,
               },
               Err(e) => BatchDataResponse {
                   request_id: req.request_id,
                   success: false,
                   error_message: e.to_string(),
                   version: req.version,
               },
           }
       }
   }
   ```

#### 9.2.2 缓存管理

1. **文件缓存**
   ```rust
   struct BatchFileCache {
       path: PathBuf,                     // 临时文件路径
       file: File,                        // 文件句柄
       received_blocks: HashSet<u32>,     // 已接收的块
   }

   impl BatchFileCache {
       async fn write_block(&mut self, index: u32, data: Vec<u8>) -> Result<()> {
           // 记录块并写入文件
           self.received_blocks.insert(index);
           self.file.seek(SeekFrom::Start((index as u64) * BLOCK_SIZE))?;
           self.file.write_all(&data)?;
           Ok(())
       }
   }
   ```

2. **内存缓存**
   ```rust
   struct BatchMemoryCache {
       blocks: HashMap<u32, Vec<u8>>,     // 块索引 -> 数据
       total_size: usize,                 // 总大小
   }

   impl BatchMemoryCache {
       async fn write_block(&mut self, index: u32, data: Vec<u8>) -> Result<()> {
           // 直接存储到内存
           self.blocks.insert(index, data);
           Ok(())
       }
   }
   ```

#### 9.2.3 注意事项

1. **并发控制**
   - 使用 MAX_CONCURRENT_TASKS 控制带宽使用
   - 通过 MAX_PENDING_BLOCKS 实现背压控制
   - 任务完成后及时释放资源

2. **内存管理**
   - 预取块数量不超过 MAX_PENDING_BLOCKS
   - 使用 Arc<[u8]> 避免数据复制
   - 大文件优先使用文件缓存

3. **错误处理**
   - 记录失败的块以便重试
   - 最多重试 MAX_RETRIES 次
   - 重试间隔为 RETRY_DELAY_MS
   - 单个任务超过 TASK_TIMEOUT_MS 自动取消

4. **性能优化**
   - 使用异步 I/O 提高并发性
   - 任务空闲时自动获取新块
   - 支持乱序处理和断点续传

5. **监控和调试**
   - 记录每个块的处理状态
   - 统计传输速率和成功率
   - 支持取消整个传输任务

### 9.3 请求方逻辑

1. **请求预处理**：
   - 生成唯一的 request_id
   - 验证数据项数量不超过 max_batch_size
   - 设置适当的超时时间

### 9.3 并行写入实现规范

#### 9.3.1 WriteSplitDataTaskGroup 设计模式
1. **基础结构设计**
   ```rust
   enum WriteSplitDataTaskGroup {
       ToFile {
           file_path: PathBuf,
           tasks: Vec<tokio::task::JoinHandle<WSResult<()>>>,
       },
       ToMem {
           shared_mem: SharedMemHolder,
           tasks: Vec<tokio::task::JoinHandle<WSResult<()>>>,
       },
   }
   ```

2. **职责划分**
   - 任务组管理：
     - 创建和初始化写入任务
     - 跟踪任务状态和完成情况
     - 提供统一的任务管理接口
   - 数据写入：
     - 文件写入使用 FileExt::write_at
     - 内存写入使用 SharedMemOwnedAccess
     - 支持并发安全的数据访问

3. **并发控制要求**
   - 文件写入：
     - 使用 tokio::task::spawn_blocking 处理 I/O
     - 通过文件偏移确保并发安全
     - 每个任务独占写入区域
   - 内存写入：
     - 使用 SharedMemOwnedAccess 保证访问安全
     - 通过 Range<usize> 隔离数据区域
     - Arc 管理共享内存生命周期

4. **错误处理规范**
   - 数据验证：
     - 检查数据块类型匹配
     - 验证数据长度一致性
     - 确保写入位置正确
   - 错误传播：
     - 使用 Result 类型传递错误
     - 支持任务级别的错误处理
     - 实现错误重试机制

#### 9.3.2 复用规范
1. **接口设计要求**
   - 提供统一的数据写入接口
   - 支持文件和内存两种模式
   - 保持与现有实现兼容

2. **数据管理规范**
   - 文件数据：
     - 使用文件偏移管理数据位置
     - 支持并发写入和随机访问
     - 实现临时文件清理
   - 内存数据：
     - 使用 SharedMemOwnedAccess 管理
     - 支持数据分片和并发访问
     - 确保内存安全释放

3. **任务管理要求**
   - 并发控制：
     - 使用信号量限制并发任务数
     - 支持任务取消和超时处理
     - 实现资源自动释放
   - 状态同步：
     - 跟踪任务完成状态
     - 支持等待所有任务完成
     - 提供任务进度反馈

4. **性能优化准则**
   - 预分配资源：
     - 文件空间预分配
     - 内存缓冲区预分配
     - 任务队列容量预设
   - 并发调优：
     - 根据系统资源调整并发度
     - 优化任务调度策略
     - 减少数据复制开销

## 10. 构建规则

### 10.1 编译命令规范

#### 10.1.1 使用 sudo 编译
- 项目编译前必须确保已设置默认工具链：
  ```bash
  rustup default stable
  ```

- 项目编译必须使用 sudo 权限：
  ```bash
  sudo -E $HOME/.cargo/bin/cargo build
  ```

#### 10.1.2 使用场景
1. 首次编译项目
2. 依赖更新后的完整编译
3. 涉及系统级权限的功能修改

#### 10.1.3 安全注意事项
1. 确保使用 sudo 的必要性：
   - 仅在确实需要系统权限时使用
   - 优先考虑其他解决方案

2. 权限管理：
   - 确保开发者具有必要的 sudo 权限
   - 遵循最小权限原则
   - 避免在非必要情况下使用 sudo

3. 环境一致性：
   - 保持开发环境权限配置一致
   - 记录所有需要 sudo 权限的依赖
   - 在文档中说明使用 sudo 的原因

4. 编译环境检查：
   - 确保 rustup 工具链已正确安装
   - 确保已设置默认工具链：`rustup default stable`
   - 检查 cargo 路径是否正确

### 8.3 处理方逻辑

1. **并发处理**：
   - 使用工作池处理批量请求
   - 控制并发度
   - 实现公平调度

2. **资源管理**：
   - 内存使用限制
   - 连接数限制
   - CPU 使用限制

3. **监控和日志**：
   - 记录处理时间
   - 记录成功/失败率
   - 记录资源使用情况

### 8.4 最佳实践

1. **批量大小**：
   - 建议单批次处理 100-1000 个数据项
   - 根据数据大小动态调整

2. **超时设置**：
   - 基础超时：30秒
   - 根据批量大小线性增加
   - 最大超时：120秒

3. **错误处理**：
   - 提供详细的错误信息
   - 支持部分成功的情况
   - 实现幂等性

4. **性能考虑**：
   - 使用异步处理
   - 实现批量压缩
   - 考虑网络带宽限制

  - 把规则视为思维框架而不是外部约束
  - 养成先检查当前上下文的习惯
  - 避免在已有信息的情况下去外部搜索
- 关注本质：
  - 理解问题的根本原因比立即解决问题更重要
  - 分析失误的思维模式而不是简单记住正确操作
  - 把经验转化为思维方式而不是操作步骤
